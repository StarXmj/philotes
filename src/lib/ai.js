import { pipeline, env } from '@huggingface/transformers'; // <-- Migration effectu√©e

// --- CONFIGURATION ---
// 1. On interdit le local pour √©viter les erreurs de chemin sur Vite/Netlify
env.allowLocalModels = false;
env.allowRemoteModels = true;

// 2. GESTION DU CACHE
// Mettez 'false' UNIQUEMENT si vous d√©veloppez et que vous avez corrompu le mod√®le.
// Pour la prod et l'exp√©rience utilisateur, il faut absolument 'true'.
env.useBrowserCache = true; 

class AIEmbedding {
  static task = 'feature-extraction';
  static model = 'Xenova/all-MiniLM-L6-v2';
  static instance = null;

  static async getInstance() {
    if (this.instance === null) {
      console.log("üöÄ D√©marrage du chargement du mod√®le IA...");
      this.instance = await pipeline(this.task, this.model, {
        progress_callback: (data) => {
          // On pourrait utiliser √ßa pour une barre de chargement globale
          if (data.status === 'progress') {
             // console.log(`Chargement mod√®le: ${Math.round(data.progress)}%`);
          }
        }
      });
      console.log("‚úÖ Mod√®le IA charg√© et pr√™t !");
    }
    return this.instance;
  }
}

// Nouvelle fonction pour pr√©charger l'IA sans bloquer
export const preloadModel = () => {
  AIEmbedding.getInstance().catch(err => console.error("Erreur pr√©chargement IA:", err));
};

export async function generateProfileVector(text) {
  const extractor = await AIEmbedding.getInstance();
  const output = await extractor(text, { pooling: 'mean', normalize: true });
  return Array.from(output.data);
}